{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machines of healing grace?\n",
    "\n",
    "Code with basic analysis and results from the AI v Covid paper\n",
    "\n",
    "**Sections**\n",
    "\n",
    "1. Descriptive analysis\n",
    "  * How much Covid and AI activity do we detect in our data sources?\n",
    "  * Is AI over or underrepresented in Covid research\n",
    "  * How has AI activity evolved over time?\n",
    "2. Topical analysis\n",
    "  * What is the topical composition of Covid research and in what areas is AI focusing?\n",
    "  * What are some examples of AI research to tackle Covid?\n",
    "  * How has it evolved over time?\n",
    "3. Geography\n",
    "  * Where is AI research happening?\n",
    "  * Who is doing it?\n",
    "  * Do we find any differences in the topics that different countries focus on?\n",
    "  * What reflects whether a country focuses on Covid research? Demand pull or supply push?\n",
    "4. **Knowledge base**\n",
    "  * On what topics do AI researchers draw on?\n",
    "4. Analysis of diffusion\n",
    "  * What determines the focus of AI researchers on particular topics?\n",
    "  * Does Covid oriented-AI research reflect the composition of the broader field? \n",
    "  * What researchers have been attracted to AI research and why?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair_saver import save\n",
    "\n",
    "from scipy.stats import entropy, zscore\n",
    "from cord19.estimators.complexity import *\n",
    "\n",
    "from data_getters.inspector import get_schemas\n",
    "from data_getters.core import get_engine\n",
    "from dotenv import load_dotenv,find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "sql_creds = os.getenv('config_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_PATH = f\"{project_dir}/reports/figures/report_1\"\n",
    "SRC_PATH = f\"{project_dir}/data/processed/ai_research\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(x):\n",
    "    print(x.head())\n",
    "    print('\\n')\n",
    "    print(x.shape)\n",
    "    \n",
    "    return(x)\n",
    "\n",
    "def herf(vector):\n",
    "    '''Calculates the herfindahl concentration index for a vector\n",
    "    \n",
    "    '''\n",
    "    return(1-np.sum([(x/sum(vector))**2 for x in list(vector)])) #NB we are doing the reverse of herf\n",
    "\n",
    "def binarise(vector,thres=0.1):\n",
    "    '''Binarises values in a vector based in whether they are above a value or not\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return([x>thres for x in list(vector)])\n",
    "\n",
    "def calculate_diversities(topic_mix,measures,measure_names):\n",
    "    '''Applies various diversity measures to a df\n",
    "    '''\n",
    "    \n",
    "    div = pd.concat([topic_mix.apply(measure,axis=1) for measure in measures],axis=1)\n",
    "\n",
    "    div.columns = measure_names\n",
    "    \n",
    "    return(div)\n",
    "    \n",
    "def div_ai_corr(div_long,ai_map,method='spearman'):\n",
    "    '''Calculates correlation between diversity measures and AI shares\n",
    "    \n",
    "    '''\n",
    "    dv = div_long.copy()\n",
    "    \n",
    "    dv['ai_share'] = dv['cluster'].map(ai_shares)\n",
    "    \n",
    "    corr = dv.pivot_table(index=['cluster','ai_share'],\n",
    "                      columns='variable',values='value').reset_index(level=1).corr(method=method)\n",
    "    return(corr)\n",
    "\n",
    "def complexity_index(x):\n",
    "    '''\n",
    "    Calculates complexity index for a group / topic mix matrix\n",
    "    '''\n",
    "    \n",
    "    lq = create_lq(x)\n",
    "    eci = calc_eci(lq)\n",
    "    return(eci)\n",
    "\n",
    "def save_fig(figure,name):\n",
    "    save(figure,f'{FIG_PATH}/{name}.png',method='selenium',\n",
    "         webdriver=DRIVER,scale_factor=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(_list,freq=False,norm=True):\n",
    "    \n",
    "    flat = [x for el in _list for x in el]\n",
    "    \n",
    "    if freq==False:\n",
    "        return flat\n",
    "    else:\n",
    "        return pd.Series(flat).value_counts(normalize=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = pd.read_csv(f\"{project_dir}/data/processed/ai_research/tidy_paper_topics_ai_2.csv\")\n",
    "\n",
    "tops['is_ai']=tops['is_ai'].astype(bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{project_dir}/data/processed/ai_research/ai_article_mag_info.json\",'r') as infile:\n",
    "    article_mag = json.load(infile)\n",
    "    \n",
    "with open(f\"{project_dir}/data/processed/ai_research/citation_lookup.json\",'r') as infile:\n",
    "    citation_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This MAG fields of study table contains information about the mag hierarchy we can use to parse it\n",
    "con = get_engine(sql_creds)\n",
    "\n",
    "mag_fos = pd.concat(pd.read_sql('mag_fields_of_study',con,chunksize=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Knowledge base of various clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process fields of study info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_fos['name_l'] = [x.lower() for x in mag_fos['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_levels = mag_fos.groupby('level')['name_l'].apply(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract cited fos by paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert mag ids to strs in the corpus df\n",
    "tops_ = tops.dropna(axis=0,subset=['mag_id'])\n",
    "\n",
    "tops_['mag_id'] = [str(int(x)) for x in tops_['mag_id']]\n",
    "\n",
    "\n",
    "cov_short = tops_[['article_id','mag_id','cluster','is_ai']].drop_duplicates(\n",
    "    'mag_id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "cov_short['cited'] = cov_short['mag_id'].map(citation_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect field of study sets for each element in cited\n",
    "cov_cits = cov_short.dropna(axis=0,subset=['cited'])\n",
    "\n",
    "cov_cits['fos_cited'] = [flatten([article_mag[x]['fields_of_study'] if 'fields_of_study' in \n",
    "                          article_mag[x].keys() else [] for x in cit]) for cit in cov_cits['cited']]\n",
    "\n",
    "cov_cits['fos_cited_unique'] = [set(x) for x in cov_cits['fos_cited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_cits['fos_cited_l1'] = [[x for x in cited if x in mag_levels[0]] for cited in cov_cits['fos_cited']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level analysis: distribution of citations at level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In total\n",
    "\n",
    "#def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For AI vs AI\n",
    "\n",
    "l1_cits = cov_cits.groupby('is_ai')['fos_cited_l1'].apply(lambda x: 100*flatten(x,freq=True)).reset_index(\n",
    "    drop=False).pipe(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = (alt\n",
    "        .Chart(l1_cits)\n",
    "        .mark_bar(opacity=0.5,stroke='black')\n",
    "        .encode(\n",
    "            y=alt.Y('level_1',sort=alt.EncodingSortField('fos_cited_l1','sum',order='descending'),\n",
    "                   title='Field of Study'),\n",
    "            x=alt.X('fos_cited_l1',stack=None,title=['% of citations by papers','in category']),color='is_ai'))\n",
    "\n",
    "out = base.properties(height=300,width=200)\n",
    "\n",
    "save_fig(out,\"fig_7_field1_citations\")\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_citation_distribution(df):\n",
    "    '''\n",
    "    Calculates distribution of citations by paper and bins them into categories\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_w_cit = df.loc[[len(x)>0 for x in df['fos_cited_l1']]].reset_index(drop=False)\n",
    "    \n",
    "    logger.info(f\"total with citations={len(df_w_cit)} and total_ai={sum(df_w_cit['is_ai']==1)}\")\n",
    "    \n",
    "    #Citation distribution by paper \n",
    "    p_f1_citations = pd.DataFrame(\n",
    "        [pd.Series(x).value_counts() for x in df_w_cit['fos_cited_l1']]).apply(\n",
    "        lambda x: x/x.sum(),axis=1).fillna(0)\n",
    "    \n",
    "    paper_cit_distr = pd.concat([df_w_cit[['is_ai','article_id']],p_f1_citations],axis=1)\n",
    "\n",
    "    paper_cit_distr_long = paper_cit_distr.melt(id_vars=['is_ai','article_id'],value_name='share_fields_cited')\n",
    "    \n",
    "\n",
    "    paper_cit_distr_long_2 = pd.concat([paper_cit_distr_long,paper_cit_distr_long.groupby(\n",
    "        ['variable','is_ai'])['share_fields_cited'].apply(\n",
    "        lambda x: pd.cut(x,bins=20,labels=False)).rename('bin').reset_index(drop=True)],axis=1)\n",
    "    \n",
    "    paper_cit_bins = paper_cit_distr_long_2.groupby(\n",
    "    ['is_ai','variable','bin'])['share_fields_cited'].size().reset_index(drop=False)\n",
    "\n",
    "    paper_cit_bins['norm'] = paper_cit_bins.groupby(['is_ai','variable'])['share_fields_cited'].apply(lambda x: x/x.sum())\n",
    "    \n",
    "    return(paper_cit_bins)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paper_cit_bins = get_field_citation_distribution(cov_cits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hist(df,t):\n",
    "    h = (alt.Chart(df)\n",
    "         .transform_filter(alt.FieldOneOfPredicate('variable',['computer science','medicine','biology','mathematics']))\n",
    "         .mark_bar(opacity=0.5,width=5,stroke='black',\n",
    "                   strokeWidth=1).encode(x='bin',y=alt.Y('norm',stack=None,title='Share of papers'),\n",
    "                               row=alt.Row('variable',sort=['computer science','medicine','biology','mathematics'],\n",
    "                                          title='Field being cited'),\n",
    "                                 color='is_ai:N')\n",
    "         .properties(height=100,width=120,title=t))\n",
    "    return(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hist = make_hist(all_paper_cit_bins,\"All papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_clusters_sorted = list(cov_short.query('is_ai==1')['cluster'].value_counts().index)\n",
    "top_5_ai_clusters = ai_clusters_sorted[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusts_distr = [get_field_citation_distribution(cov_cits.loc[cov_cits['cluster']==c]) for c in top_5_ai_clusters]\n",
    "\n",
    "hists = [all_hist]+[make_hist(cl,n) for cl,n in zip(clusts_distr,top_5_ai_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = alt.hconcat(*hists).resolve_scale(y='shared')\n",
    "\n",
    "save_fig(hists,\"fig_8_hists\")\n",
    "\n",
    "hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap with citations to group 3 by category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_cits['fos_cited_l1'] = [[x for x in cited if x in mag_levels[1]] for cited in cov_cits['fos_cited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_topics = list(flatten(cov_cits['fos_cited_l1'],freq=True)[:30].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_fos1 = cov_cits.groupby(\n",
    "    ['is_ai','cluster'])['fos_cited_l1'].apply(lambda x: 100*flatten(list(x),freq=True)).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a lookup\n",
    "fos_0_lu = {r['id']:r['name'] for idx,r in mag_fos.query(\"level == 0\").iterrows()}\n",
    "\n",
    "fos_1_to_0_lu = {fos_0_lu[[int(x) for x in pars.split(',')][0]] for pars in mag_fos.loc[mag_fos['level']==1]['parent_ids']}\n",
    "\n",
    "name_lookup = {r['name'].lower():fos_0_lu[int(r['parent_ids'].split(',')[0])] for rid,r in mag_fos.loc[mag_fos['level']==1].iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add higher level discipline to table\n",
    "\n",
    "cov_fos1['discipline'] = cov_fos1['level_2'].map(name_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_ch = (alt.Chart(cov_fos1)\n",
    " .transform_filter(alt.FieldOneOfPredicate('level_2',top_50_topics))\n",
    " .transform_filter(alt.FieldOneOfPredicate('cluster',ai_clusters_sorted[:10]))\n",
    " .mark_point(filled=True,strokeWidth=0.7,stroke='black')\n",
    " .encode(y='is_ai',\n",
    "         x=alt.X('level_2',sort=top_50_topics),\n",
    "         size=alt.Size('fos_cited_l1',title=['% of all citations','in category']),\n",
    "         color='discipline:N',\n",
    "      row=alt.Row('cluster',sort=ai_clusters_sorted[:10]))).properties(width=500)\n",
    "\n",
    "point_ch = point_ch.configure_axis(grid=True)\n",
    "\n",
    "save_fig(point_ch,\"fig_9_bubble\")\n",
    "\n",
    "point_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
