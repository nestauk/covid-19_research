{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI topic exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cord19.transformers.nlp_2 import *\n",
    "from toolz.curried import *\n",
    "import altair as alt\n",
    "from altair_saver import save\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(x):\n",
    "    print(x.head())\n",
    "    print(x.shape)\n",
    "    return(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xiv = pd.read_csv(f\"{project_dir}/data/processed/ai_research/xiv_papers_labelled.csv\").pipe(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = xiv.query(\"is_AI == 1\").reset_index(drop=False).pipe(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = pd.read_csv(f\"{project_dir}/data/processed/ai_research/tidy_paper_topics_ai.csv\").pipe(preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "* pre-process AI text\n",
    "* Train topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and tokenise the AI data\n",
    "#Remove line breaks\n",
    "abstr_clean = [re.sub(\"\\n\",\" \",x).strip() for x in ai['abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanTokenize(abstr_clean)\n",
    "ct.clean().bigram().bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a topic model with eg 100 topics\n",
    "\n",
    "lda = LdaPipeline(ct.tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.filter().process().fit_lda(num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.lda_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.predict_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5\n",
    "topic_word_mixes = [x[1].split(\"+\") for x in lda.lda_topics]\n",
    "topic_names = [\"_\".join([re.sub('\"','',w.split(\"*\")[1].strip()) for w in x][:num_words]) for x in topic_word_mixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_year_map = ai.set_index('id')['year'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = lda.predicted_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.columns = topic_names\n",
    "topic_df['id'],topic_df['mag_id'] = ai['id'],ai['mag_id']\n",
    "\n",
    "topic_df['year'] = topic_df['id'].map(ai_year_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_long = topic_df.melt(id_vars=['id','mag_id','year'],var_name='topic',value_name='weight'\n",
    "                          ).pipe(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_long.to_csv(f\"{project_dir}/data/processed/ai_research/ai_topics.csv\",index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra labels for analysis\n",
    "cov_ids = set(cov['index'])\n",
    "cov_lookup = cov.set_index('index')['cluster'].to_dict()\n",
    "\n",
    "\n",
    "topic_long['is_covid'] = topic_long['id'].apply(lambda x: x in cov_ids)\n",
    "topic_long['cluster'] = topic_long['id'].map(cov_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we subset the long topic df to focus on papers with some topic presence\n",
    "thres = 0.01\n",
    "\n",
    "#We are focusing on papers published recently\n",
    "topic_long_recent = topic_long.query(\"year > 2019\")\n",
    "\n",
    "#We will use these topics for normalisation\n",
    "totals = topic_long_recent.drop_duplicates('id')['is_covid'].value_counts()\n",
    "\n",
    "papers_with_topic = topic_long_recent.query(f\"weight > {thres}\").reset_index(drop=True).pipe(preview)\n",
    "topic_distr = papers_with_topic.groupby(['topic','is_covid'])['weight'].sum().reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We focus on variables of interest\n",
    "topic_distr_wide = topic_distr.pivot_table(index='topic',columns='is_covid',values='weight').sort_values(\n",
    "    True,ascending=False).pipe(preview)\n",
    "\n",
    "#This normalises the topics by total numbers of paper in a category\n",
    "top_distr_norm = (100*(topic_distr_wide/totals)).reset_index(drop=False).melt(id_vars='topic').pipe(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the papers with the biggest deltas between AI and non-AI\n",
    "top_deltas = (topic_distr_wide/totals).assign(delta = lambda x: abs(x[True]-x[False])\n",
    "                                             ).sort_values('delta',ascending=False).pipe(preview)\n",
    "\n",
    "top_deltas['max'] = 100*top_deltas.iloc[:,:-1].max(axis=1).pipe(preview)\n",
    "\n",
    "top_differences = top_deltas[:10].index\n",
    "\n",
    "#Add the maximum value for a topic to help with the labelling later\n",
    "top_distr_norm['max'] = top_distr_norm['topic'].map(top_deltas['max'].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the chart\n",
    "base = alt.Chart(top_distr_norm).encode(x=alt.X('topic',sort=list(top_distr_norm.index),\n",
    "                                               axis=alt.Axis(labels=False)),tooltip=['topic'])\n",
    "\n",
    "p = base.mark_point(filled=True).encode(y='value',color='is_covid:N')\n",
    "\n",
    "c = base.mark_line(strokeWidth=1,color='darkgrey',strokeDash=[1,1]).encode(y='value',detail='topic')\n",
    "\n",
    "t = (base\n",
    "     .transform_filter(alt.FieldOneOfPredicate('topic',list(top_differences)))\n",
    "     .mark_text(align='left',fontSize=10,angle=0,xOffset=2,yOffset=-5,color='black',opacity=0.8)\n",
    "     .encode(text='topic',y=alt.Y('max',title='% of papers with topic')))\n",
    "\n",
    "out = (p + c + t).properties(width=400,height=450)\n",
    "\n",
    "save(out,\"test.png\",method='selenium',\n",
    "         webdriver=DRIVER,scale_factor=2)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_res = []\n",
    "\n",
    "#Loops over the topic names and tests differences in means\n",
    "for n,t in enumerate(topic_names):\n",
    "    \n",
    "    df_in_topic = topic_long_recent.loc[[x==t for x in topic_long_recent['topic']]]\n",
    "    \n",
    "    ttest = ttest_ind(df_in_topic.query(f'is_covid == True')['weight'],\n",
    "                     df_in_topic.query(f'is_covid == False')['weight'])\n",
    "    \n",
    "    ttest_res.append(ttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest = pd.DataFrame([pd.Series(\n",
    "    {'t_stat_abs':abs(x[0]),'p_val':x[1],'higher_group': 'covid' if x[0]>0 else 'non_covid'},name=t) for t,x in zip(topic_names,ttest_res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_sort = rest.sort_values('t_stat_abs',ascending=False).reset_index(drop=False).iloc[:30].melt(\n",
    "    id_vars=['index','higher_group'])\n",
    "\n",
    "alt.Chart(rest_sort).mark_bar().transform_filter(alt.datum.variable=='t_stat_abs').encode(\n",
    "    y=alt.Y('index',sort=alt.EncodingSortField('value',order='descending')),\n",
    "    x=alt.X('value',title='t-statistic (absolute)'),\n",
    "    color='higher_group').properties(width=200,height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xiv.columns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
