{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity analysis\n",
    "\n",
    "We consider if fields with higher levels of AI activity have more knowledge modularity than those that don't.\n",
    "\n",
    "Actions:\n",
    "1. Load relevant data\n",
    "  * topic mix\n",
    "  * citation data\n",
    "2. Exploratory analysis\n",
    "  * Knowledge basis for other clusters\n",
    "2. Analyse modularity\n",
    "  * Topic distribution in different clusters\n",
    "  * Modularity of citation network for each cluster\n",
    "  * Modularity of topics leveraged in each cluster\n",
    "  \n",
    "The above will probably require a function to extract networks from co-occurrence item lists and an algorithm to calculate the modularity of those networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from altair_saver import save\n",
    "from selenium import webdriver\n",
    "\n",
    "from scipy.stats import entropy, zscore\n",
    "from cord19.estimators.complexity import *\n",
    "\n",
    "from data_getters.inspector import get_schemas\n",
    "from data_getters.core import get_engine\n",
    "from dotenv import load_dotenv,find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "sql_creds = os.getenv('config_path')\n",
    "\n",
    "driver_path = os.getenv('chrome_driver_path')\n",
    "DRIVER = webdriver.Chrome(executable_path=driver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(x):\n",
    "    print(x.head())\n",
    "    print('\\n')\n",
    "    print(x.shape)\n",
    "    \n",
    "    return(x)\n",
    "\n",
    "def herf(vector):\n",
    "    '''Calculates the herfindahl concentration index for a vector\n",
    "    \n",
    "    '''\n",
    "    return(1-np.sum([(x/sum(vector))**2 for x in list(vector)])) #NB we are doing the reverse of herf\n",
    "\n",
    "def binarise(vector,thres=0.1):\n",
    "    '''Binarises values in a vector based in whether they are above a value or not\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return([x>thres for x in list(vector)])\n",
    "\n",
    "def calculate_diversities(topic_mix,measures,measure_names):\n",
    "    '''Applies various diversity measures to a df\n",
    "    '''\n",
    "    \n",
    "    div = pd.concat([topic_mix.apply(measure,axis=1) for measure in measures],axis=1)\n",
    "\n",
    "    div.columns = measure_names\n",
    "    \n",
    "    return(div)\n",
    "    \n",
    "def div_ai_corr(div_long,ai_map,method='spearman'):\n",
    "    '''Calculates correlation between diversity measures and AI shares\n",
    "    \n",
    "    '''\n",
    "    dv = div_long.copy()\n",
    "    \n",
    "    dv['ai_share'] = dv['cluster'].map(ai_shares)\n",
    "    \n",
    "    corr = dv.pivot_table(index=['cluster','ai_share'],\n",
    "                      columns='variable',values='value').reset_index(level=1).corr(method=method)\n",
    "    return(corr)\n",
    "\n",
    "def complexity_index(x):\n",
    "    '''\n",
    "    Calculates complexity index for a group / topic mix matrix\n",
    "    '''\n",
    "    \n",
    "    lq = create_lq(x)\n",
    "    eci = calc_eci(lq)\n",
    "    return(eci)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(_list,freq=False,norm=True):\n",
    "    \n",
    "    flat = [x for el in _list for x in el]\n",
    "    \n",
    "    if freq==False:\n",
    "        return flat\n",
    "    else:\n",
    "        return pd.Series(flat).value_counts(normalize=norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tops = pd.read_csv(f\"{project_dir}/data/processed/ai_research/tidy_paper_topics_ai.csv\")\n",
    "\n",
    "tops['is_ai']=tops['is_ai'].astype(bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{project_dir}/data/processed/ai_research/ai_article_mag_info.json\",'r') as infile:\n",
    "    article_mag = json.load(infile)\n",
    "    \n",
    "with open(f\"{project_dir}/data/processed/ai_research/citation_lookup.json\",'r') as infile:\n",
    "    citation_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This MAG fields of study table contains information about the mag hierarchy we can use to parse it\n",
    "con = get_engine(sql_creds)\n",
    "\n",
    "mag_fos = pd.concat(pd.read_sql('mag_fields_of_study',con,chunksize=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Link between AI share and topic concentration\n",
    "\n",
    "* Do research fields with more AI activity tend to have less topic diversity / complexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prep\n",
    "div_measures = [herf,entropy]\n",
    "div_measure_names = ['herfindahl','entropy']\n",
    "\n",
    "#paper-cluster lookup\n",
    "paper_cluster_lu = tops.drop_duplicates('index').set_index('index')['cluster'].to_dict()\n",
    "\n",
    "#cluster-share_lookup\n",
    "ai_shares = tops.drop_duplicates('index').groupby('cluster')['is_ai'].mean().sort_values(\n",
    "    ascending=False).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus on papers\n",
    "paper_mixes = tops.pivot_table(index='index',columns='topic',values='weight')\n",
    "\n",
    "#Calculate diversify measures\n",
    "div = calculate_diversities(paper_mixes,div_measures,div_measure_names)\n",
    "\n",
    "#Allocate papers to clusters and calculate means\n",
    "div['cluster'] = div.index.map(paper_cluster_lu)\n",
    "\n",
    "div_means = div.groupby('cluster').mean().apply(zscore).reset_index(drop=False).melt(id_vars=['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_ai_corr(div_means,ai_shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate field level diversities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bin = paper_mixes.applymap(lambda x: int(x>0.1))\n",
    "#Label papers with clusters\n",
    "top_bin['cluster'] = top_bin.index.map(paper_cluster_lu)\n",
    "\n",
    "top_bin_long = top_bin.melt(id_vars='cluster')\n",
    "\n",
    "cl_top_distr = top_bin_long.groupby(['cluster','topic'])['value'].sum().reset_index(\n",
    "    name='count').pivot(index='cluster',columns='topic',values='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_div = calculate_diversities(cl_top_distr,[herf,entropy],['herfindahl','entropy']).reset_index(\n",
    "    drop=False).melt(id_vars='cluster')\n",
    "\n",
    "eci = complexity_index(cl_top_distr).reset_index(drop=False).melt(id_vars='cluster')\n",
    "\n",
    "cl_div_2 = pd.concat([cl_div,eci])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_ai_corr(cl_div_2,ai_shares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Knowledge base of various clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process fields of study info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_fos['name_l'] = [x.lower() for x in mag_fos['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_levels = mag_fos.groupby('level')['name_l'].apply(set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract cited fos by paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert mag ids to strs in the corpus df\n",
    "tops_ = tops.dropna(axis=0,subset=['mag_id'])\n",
    "\n",
    "tops_['mag_id'] = [str(int(x)) for x in tops_['mag_id']]\n",
    "\n",
    "\n",
    "cov_short = tops_[['index','mag_id','cluster','is_ai']].drop_duplicates(\n",
    "    'mag_id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "cov_short['cited'] = cov_short['mag_id'].map(citation_lookup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect field of study sets for each element in cited\n",
    "cov_cits = cov_short.dropna(axis=0,subset=['cited'])\n",
    "\n",
    "cov_cits['fos_cited'] = [flatten([article_mag[x]['fields_of_study'] if 'fields_of_study' in \n",
    "                          article_mag[x].keys() else [] for x in cit]) for cit in cov_cits['cited']]\n",
    "\n",
    "cov_cits['fos_cited_unique'] = [set(x) for x in cov_cits['fos_cited']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_cits['fos_cited_l1'] = [[x for x in cited if x in mag_levels[0]] for cited in cov_cits['fos_cited']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level analysis: distribution of citations at level 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In total\n",
    "\n",
    "#def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For AI vs AI\n",
    "\n",
    "l1_cits = cov_cits.groupby('is_ai')['fos_cited_l1'].apply(lambda x: 100*flatten(x,freq=True)).reset_index(\n",
    "    drop=False).pipe(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = (alt\n",
    "        .Chart(l1_cits)\n",
    "        .mark_bar(opacity=0.5,stroke='black')\n",
    "        .encode(\n",
    "            y=alt.Y('level_1',sort=alt.EncodingSortField('fos_cited_l1','sum',order='descending')),\n",
    "            x=alt.X('fos_cited_l1',stack=None,title='% of fields cited'),color='is_ai'))\n",
    "\n",
    "out = base.properties(height=300,width=200)\n",
    "\n",
    "save(out,\"test.png\",method='selenium',\n",
    "         webdriver=DRIVER,scale_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_w_cit = cov_cits.loc[[len(x)>0 for x in cov_cits['fos_cited_l1']]].reset_index(drop=False)\n",
    "\n",
    "p_f1_citations = pd.DataFrame(\n",
    "    [pd.Series(x).value_counts() for x in cov_w_cit['fos_cited_l1']]).apply(\n",
    "    lambda x: x/x.sum(),axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_cit_distr = pd.concat([cov_w_cit[['is_ai','cluster']],p_f1_citations],axis=1)\n",
    "\n",
    "paper_cit_distr_long = paper_cit_distr.melt(id_vars=['is_ai','cluster'],value_name='share_fields_cited').pipe(\n",
    "    preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_cit_distr_long_2 = pd.concat([paper_cit_distr_long,paper_cit_distr_long.groupby(\n",
    "    ['variable','is_ai'])['share_fields_cited'].apply(\n",
    "    lambda x: pd.cut(x,bins=20,labels=False)).rename('bin').reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_cit_bins = paper_cit_distr_long_2.groupby(\n",
    "    ['is_ai','variable','bin'])['share_fields_cited'].size().reset_index(drop=False)\n",
    "\n",
    "paper_cit_bins['norm'] = paper_cit_bins.groupby(['is_ai','variable'])['share_fields_cited'].apply(lambda x: x/x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(alt.Chart(paper_cit_bins)\n",
    " .transform_filter(alt.FieldOneOfPredicate('variable',['computer science','medicine','biology','mathematics']))\n",
    " .mark_bar(opacity=0.5,width=7,stroke='black',\n",
    "           strokeWidth=1).encode(x='bin',y=alt.Y('norm',stack=None,title='Share of papers'),\n",
    "                               row='variable',color='is_ai:N')\n",
    " .properties(\n",
    "    height=100,width=170))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fos_cited(df,unique):\n",
    "    '''Extracts citations from a subcorpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if unique!=False:\n",
    "        return(flatten(flatten(df['fos_cited']),freq=True))\n",
    "    else:\n",
    "        return(flatten(df['fos_cited_unique'],freq=True,norm=True))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_freqs = {}\n",
    "cluster_freqs_unique = {}\n",
    "\n",
    "for x in set(tops['cluster']):\n",
    "    \n",
    "    if pd.isnull(x)==False:\n",
    "    \n",
    "        rel = cov_cits.loc[cov_cits['cluster']==x]\n",
    "        freqs = extract_fos_cited(rel,unique=False)\n",
    "    \n",
    "        cluster_freqs[x] = freqs\n",
    "        \n",
    "        #This calculates the share of papers in a cluster that cite a topic\n",
    "        freqs_unique = 100*extract_fos_cited(rel,unique=True)\n",
    "        cluster_freqs_unique[x] = freqs_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_mix = pd.DataFrame(cluster_freqs_unique)\n",
    "top_fos = field_mix.sum(axis=1).sort_values(ascending=False)[:50].index\n",
    "field_mix_long= field_mix.loc[top_fos].reset_index(drop=False).melt(id_vars='index',var_name='cluster',\n",
    "                                                                      value_name='share')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "\n",
    "(alt.Chart(field_mix_long)\n",
    " .mark_rect()\n",
    " .encode(\n",
    "    x=alt\n",
    "     .X('cluster:N',sort=list(ai_shares.keys())),\n",
    "    y=alt\n",
    "     .Y('index',sort=alt.EncodingSortField('share',op='sum',order='descending')),\n",
    "    color='share:Q',tooltip=['cluster','index'])\n",
    " .properties(height=550))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check this\n",
    "\n",
    "cl_11 = cov_cits.loc[cov_cits['cluster']=='cluster_11']\n",
    "\n",
    "cl_11_res = {}\n",
    "\n",
    "for x in [True,False]:\n",
    "    \n",
    "    rel_2 = cl_11.loc[cl_11['is_ai']==x]\n",
    "    out = extract_fos_cited(rel_2,unique=True)\n",
    "    cl_11_res[x]=out\n",
    "    \n",
    "check = pd.DataFrame(cl_11_res).fillna(0).sort_values(False,ascending=False)\n",
    "\n",
    "top_f = check.index[:40]\n",
    "\n",
    "check_long = pd.melt(check.loc[[x in top_f for x in check.index]].reset_index(drop=False),id_vars='index')\n",
    "\n",
    "bas = alt.Chart(check_long).mark_bar(opacity=0.4).encode(\n",
    "    y=alt.Y('index',sort=list(top_f)))\n",
    "\n",
    "ai = bas.transform_filter(alt.datum.variable==True).encode(x='value',color='variable')\n",
    "\n",
    "nai = bas.transform_filter(alt.datum.variable==False).encode(x='value',color='variable')\n",
    "\n",
    "(ai+nai).properties(height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
