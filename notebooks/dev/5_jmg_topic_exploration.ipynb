{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical topic modelling\n",
    "\n",
    "Here we fit a hierarchical topic model on the covid-19 paper data. We are specially interested in clustering papers into groups and then analyse their topics\n",
    "\n",
    "Steps:\n",
    "\n",
    "* Pre-process the data\n",
    "* Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cord19.hSBM_Topicmodel.sbmtm import sbmtm\n",
    "from cord19.transformers.nlp_2 import *\n",
    "\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from toolz.curried import *\n",
    "import altair as alt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(x):\n",
    "    print(x.head())\n",
    "    print('\\n')\n",
    "    print(x.shape)\n",
    "    \n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the covid article data\n",
    "cov_ = pd.read_csv(f\"{project_dir}/data/processed/covid_df.csv\").pipe(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ids for AI papers based on previous analysis\n",
    "ai_ids = set(pd.read_csv(f\"{project_dir}/data/raw/ai_research/ai_paper_ids.csv\")['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop papers without abstracts\n",
    "cov_ = cov_.dropna(axis=0,subset=['abstract']).pipe(preview)\n",
    "\n",
    "cov = cov_.loc[[len(x)>300 for x in cov_['abstract']]].pipe(preview)\n",
    "\n",
    "cov.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_magid_lookup = {r['id']:r['mag_id'] for rid,r in cov.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Clean and tokenise the data\n",
    "\n",
    "# abst = cov['abstract']\n",
    "\n",
    "# abst = [re.sub(\"\\n\",\" \",x) for x in abst]\n",
    "\n",
    "# ct = CleanTokenize(abst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct.clean().bigram(threshold=20).bigram(threshold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = ct.tokenised\n",
    "# titles = list(cov['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# model = sbmtm()\n",
    "# model.make_graph(docs,documents=titles)\n",
    "# model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save model\n",
    "\n",
    "# with open(f\"{project_dir}/models/top_sbm/top_sbm.p\",'wb') as outfile:\n",
    "#     pickle.dump(model,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{project_dir}/models/top_sbm/top_sbm.p\",'rb') as infile:\n",
    "    model = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the word mix (word components of each topic)\n",
    "word_mix = model.topics(l=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tidier names\n",
    "topic_name_lookup = {key:'_'.join([x[0] for x in values[:5]]) for key,values in word_mix.items()}\n",
    "topic_names = list(topic_name_lookup.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the topic mix df\n",
    "topic_mix_ = pd.DataFrame(model.get_groups(l=0)['p_tw_d'].T,\n",
    "                        columns=topic_names,index=list(cov['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove highly uninformative / generic topics\n",
    "\n",
    "topic_prevalence = topic_mix_.applymap(lambda x: x>0).mean().sort_values(ascending=False)\n",
    "\n",
    "topic_prevalence.loc[topic_prevalence>0.4]\n",
    "\n",
    "filter_topics = topic_prevalence.index[topic_prevalence<0.4]\n",
    "\n",
    "topic_mix = topic_mix_[filter_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the clusters to which different documents belong (we force all documents to belong to a cluster)\n",
    "cluster_assigment = model.clusters(l=1,n=len(list(cov['id'])))\n",
    "cluster_sets = {c:set([x[0] for x in papers]) for c,papers in cluster_assigment.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign topics to their clusters\n",
    "#Add AI dummy and cluster dummy\n",
    "topic_mix['is_ai'] = [x in ai_ids for x in topic_mix.index]\n",
    "\n",
    "topic_mix['cluster'] = [[f'cluster_{n}' for n,v in cluster_sets.items() if x in v][0] for x in topic_mix.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mix_long = topic_mix.reset_index().melt(id_vars=['index','is_ai','cluster'],\n",
    "                                                            var_name='topic',value_name='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mix_long['mag_id'] = topic_mix_long['index'].map(id_magid_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mix_long.rename(columns={'index':'article_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mix_long.to_csv(f\"{project_dir}/data/processed/ai_research/tidy_paper_topics_ai_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_mix['cluster'].to_csv(f\"{project_dir}/data/processed/ai_research/paper_cluster.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
